---
title: "Ideal Point Estimation of Voters Using Cast Vote Records"
author: "Mason Reece"
date: today
date-format: long
bibliography: "references.bib"
linestretch: 1.5
abstract-title: Abstract
thanks: | 
  This paper was completed as part of the Quantitative Research Methods III: Generalized Linear Models and Extensions course taught by Teppei Yamamoto and the Bayesian Measurement Models taught by Devin Caughey. I am indebted to feedback from both of them on this project. Additionally, I acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper.
abstract: |
  How representative are politicians of their constituents? This question has been asked, and answered, for decades of political science research, but I take a new approach to the question using cast vote records (CVRs). CVRs avoid some of the flaws of survey research and aggregate-level analysis, allowing estimation of the complete distribution of voter preferences. They additionally allow me to make novel observations of the performance of lower-ballot politicians. I use this data to conduct ideal-point estimation in the state of Colorado and show that, like other research has found, that politicians are not representative of even the voters who elect them. 
execute: 
  echo: false
  cache: true
  message: false
  warning: false
format: 
  html:
    toc: true
    embed-resources: true
  pdf:
    toc: false
    number-sections: true
    citecolor: blue
    link-citations: true
    fig-cap-location: bottom
---

```{r}
#| cache: false

set.seed(02139)

library(patchwork)
library(tidyverse)
library(brms)
library(tidybayes)
library(bayesplot)
library(targets)

source("../medsl_theme.R")

```

# Introduction

How representative is the government of its constituents? This topic has been repeatedly probed in the political science literature [e.g., @caneswrone_out_2002; @bafumi_leapfrog_2010; @kirkland_representation_2022]. One of the many ways that representation is studied in American politics is by comparing the ideologies of politicians those voters who they represent. For politicians, several data sources have been used, that all show high correlation: roll-call votes [@poole_patterns_1991], campaign contributions [@bonica_mapping_2014; @bonica_inferring_2018], and tweets [@barberá2015], among others. These methods rely on the spate of data available about politicians, and their public positions necessitates that much of it be made public to researchers to use in analyses. This is not the case for voters.

The research on voters often uses aggregated measures of voter preferences or survey-based estimates of ideology rather than the actual votes of individuals [for an exception, see @lewis2001]. Survey-based estimates suffer from two related flaws. First, like all surveys, issues of sampling bias, response bias, their temporal relationship with the actual election, and other such problems prevents them from being perfect representations of vote choice. Second, surveys are often unable to ask voters about the full set of choices on their ballot, mostly given space constraints and the difficulty that arises from determining which exact races a voter was eligible to vote in. Aggregated data overcomes these two issues by using the results from an election and being able to look at the results in every race on the ballot. However, with aggregated data we lose the richness of being able to study the same voter up and down the ballot. What if voters are splitting their tickets, vacillate randomly between candidates, or otherwise act in such a way that aggregate measures would miss?

To avoid these problems, I use an original data set of "cast vote records" (CVRs) that reveal anonymous, individual voter choices in each race in the 2020 election. Since these are true votes by people, they do not suffer from the same problems as surveys, and because I can uniquely identify the same voter up and down the ballot, issues of aggregation do not apply here. The downside of using CVRs is that they reveal very little additional information about the voter. There are no names, no voter IDs, no demographic information, or anything that might violate the privacy of a voter. The only thing that could be done would be to attempt to parse the precinct of the voter (which is only available for some counties) and back out aggregate level demographic information on the voters. For this paper, I set this process aside and only work with the anonymized data.

Using this data, I estimate the latent traits of voters that are best described by their vote choices in the election. In previous research, the first dimension of this latent trait is often assumed to be the ideology of the voter, so I refer to this latent trait as ideology henceforth [@clinton2004; @lewis2001; @bonica_mapping_2014; @heckman1996; @bonica_inferring_2018]. These latent traits are interesting in their own right, but I also take the additional step where data is available to compare DIME score estimates of candidate ideology [@bonica_mapping_2014] to aggregate levels of candidate ideology I construct based on those voters who cast their ballots for that candidate. I find that those values are not perfectly correlated, indicating that candidate's behavior while in office is not representative of even those voters who elected them. This research adds to decades of research showing how representative's do not always represent the underlying distribution of beliefs in their districts [@fenno1978; @bafumi_leapfrog_2010].

# Data & Empirical Approach

My data consists of cast vote records (CVRs) collected as part of my summer research with the MIT Election Data & Science Lab (paper forthcoming). CVRs are anonymized records of which choice in each election each voter in the jurisdiction chose on their ballot. For example, a CVR in Colorado would include, for each voter, their choice for President, for the Senate race, for the House district race that voter was located in, and a number of choices in local-level races that the voter was eligible to vote in. The data is fully anonymous and I am not able to identify anything about the voters (although see @kuriwaki2023secret for a cautionary note). This data has been standardized into a common format across the nation.

CVRs were released by some local election administrators in response to increasing calls from election auditors to be able to independently verify the results of the 2020 election. Some states prohibit releasing CVRs in any form, and for other states the ability to do so depended on whether a group requested them from a certain county, whether the county used technology enabling this data to be easily compiled, and whether the county election office had the capacity to even complete the request. It is not known exactly why each county in my data released their data, and so at best this data should be treated as a non-random sample of CVRs. Nevertheless, it still contains almost 1 billion choices in elections at all levels of government and in localities of all kinds from all over the country, representing the choices of roughly 50 million voters. See @fig-map for the full distribution of counties. I remove all uncontested races, and all races where a voter could select more than one candidate (although see @sec-potential-extensions). In addition, I subset my data to a random selection of $100,000$ voters and $2.3$ million choices in Colorado, to make computation tractable. Colorado has some of the best coverage of any state in the data -- nearly every county is represented and the state already has a decent partisan and demographic mix so that I think of it as a good benchmark for how the model would perform in every other state.

![Cast Vote Record Distribution](writing/images/county_map2.jpeg){#fig-map}

# Model Estimation

Since I am attempting to estimate a continuous, latent, scale using only discrete information on each voter, I follow in the footsteps of other researchers in this topic and estimate an Item-Response Theory (IRT) model [@lewis2001; @clinton2004; @martin2002]. A common specification of the IRT model, reformulated from @jackman2009, is the "two-parameter" model. Subscripts and quantities of interest can be found in @tbl-terms.

$$
\pi_{jk} = Pr(y_{jk} = c | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - beta_{k(c)})
$$ {#eq-irt}

| Quantity                       | Symbol           |
|--------------------------------|------------------|
| Individual                     | $j = 1, 2, …, J$ |
| Race                           | $k = 1, 2, …, K$ |
| Candidate                      | $c = 1, 2, …, C$ |
| Ideal point                    | $\alpha_j$       |
| Discrimination/Slope Parameter | $\gamma_{k(c)}$  |
| Difficulty/Location Parameter  | $\beta_{k(c)}$   |

: Reference for Relevant Model Quantities and Symbols {#tbl-terms}

Of particular note are the parameters. $\alpha_j \in \mathbb{R}$ is an unobserved attribute of individual $j$ , which is typically considered to be ideology in the election context. $\gamma_{k(c)}$ is an unobserved parameter representing the *item discrimination* of candidate $c$ in race $k$, which is the extent to which the probability of voting for a certain candidate responds to changes in the latent trait $\alpha_j$. When $\gamma_{k(c)} = 1$, this is referred to as the Rasch model. $\beta_{k(c)}$ is an unknown *item difficulty* parameter which just tells us the probability of a certain candidate being chosen, irrespective of the underlying trait. In this context, this is the proportion of the votes that each candidate received. $F(\cdot)$ is a function mapping the equation to the probability line. Most IRT applications, including those used in this context in the past, work with binary data, which simplifies the probability statement above to just be testing whether $y_{jk} = 1$. I begin by manipulating my data to estimate this model as well, but I then also extend the field to work with unordered categorical data, where $y_{jk} = c$.

## Binary Outcomes

To start, I follow the previous literature on this topic @lewis2001 by only focusing on binary choices. That research achieved that by only focusing on propositions, which are inherently encoded as Yes/No choices. However, I want to make use of the full set of choices a voter made in the election, so I create a variable that is a binary 1/0 for if the voter has selected the Republican candidate in the race. This choice means that all ideal points on the right side of the scale will indicate greater likelihood to select the Republican candidate (and thus naturally map to the left/right US political party scale). In theory, a similar variable for the choice of the Democrat would have achieved basically the same results (just with the scale flipped). @eq-irt then becomes

$$
\pi_{jk} = Pr(y_{jk} = 1 | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - \beta_{k(c)})
$$

For the binary model, I set $F(\cdot)$ to be the inverse logistic function. The likelihood can then be expressed as below, given the common independence assumption across voters and races [@clinton2004]. In addition, because there are no unique candidate effects, only effects at the race level, I can drop the indexing by candidate within each race and only reference the race itself.

$$
\mathcal{L} = \prod_{j=1}^J \prod_{k=1}^K \pi_{jk}^{y_{jk}} (1 - \pi_{jk})^{1 - y_{jk}}
$$

As it stands, the model is not identified. Simply put, the scales of $\alpha$ are not set and it can be easily multiplied by any factor or shifted by any constant and the model will behave just the same. Similarly, the sign of $\gamma$ can vacillate with a corresponding switch in the sign of $\alpha$ and result in the same behavior. Therefore, a number of identification restrictions must be imposed. For a more full discussion of these conditions, see @jackman2009, @clinton2004, and @rivers2003. I choose to normalize the latent trait to mean 0 and standard deviation 1, which I impose both in post-processing and by setting a strong standard normal prior. I also impose the restriction that $\gamma$ must always be positive, thus fixing its sign.

I start by fitting a simple Rasch model. I estimate the following Bernoulli model using `brms`.[^1] All models are run for 4 chains, with 1000 warm-up iterations and then 1000 sampling iterations. Trace plots are too numerous to display, so @fig-rhats instead plots the $\hat{R}$ value for every parameter in the model. As can be seen in the first two panels, all $\hat{R}$ values are extremely close to 1, indicating the model has converged well.

[^1]: Relevant code snippets can be found in @sec-code.

```{r}
#| label: fig-rhats
#| fig-cap: Distribution of Gelman-Rubin Diagnostic

ber_1pl <- readRDS("fits/bernoulli_rasch.rds")
ber_2pl <- readRDS("fits/bernoulli_2pl.rds")

signs <- readRDS("fits/cat_2pl_unrestricted.rds") |> 
  as_draws_df() |>
  select(matches("^gamma\\[")) |>
  rowMeans() |>
  sign()

cat_2pl <- readRDS("fits/cat_2pl_unrestricted.rds") |> 
  as_draws_df() |>
  mutate(across(matches("(^alpha\\[)|(^gamma\\[)"), ~ signs * .))

fits <- list("Categorical" = cat_2pl,
             "Bernoulli, Rasch" = ber_1pl,
             "Bernoulli, 2PL" = ber_2pl)

rhats <- tibble(fit = fits, fit_name = names(fits)) |> 
  mutate(rhat = map(fit, ~ select(summarise_draws(.x), rhat))) |> 
  select(-fit) |> 
  unnest(cols = rhat)

rhats |> 
  ggplot(aes(x = rhat)) +
  geom_dots() +
  scale_y_continuous(labels = NULL) +
  facet_wrap(~ fit_name, scales = "free_x", ncol=3) +
  labs(x = expression(hat(R)), y = "", color = "Fit") +
  theme_bw()

```

Rasch models treat each race as equally important in determining the ideal point of a voter, so I only look at those ideal points. I randomly select 10 voters and plot their estimated ideal points in @fig-bernoulli-ideals. These estimates generally make sense; voters 42 and 4 voted straight-ticket Democrat and voters 45 and 39 voted straight-ticket Republican, whereas voter 7 cast a more split-ticket. The different exact estimates are due to the differing impact of casting a ballot for the Republican in a specific race versus another, which is driven entirely by the difficulty parameter in the Rasch model but by both the difficulty and discrimination parameter in the 2PL model.

```{r}
#| label: fig-bernoulli-ideals
#| fig-cap: Example Ideal Points from the Bernoulli Model
#| fig-width: 8
#| fig-height: 6

p1 <- ber_1pl |>
   spread_draws(r_cvr_id[cvr_id, var]) |>
   mutate(alpha = r_cvr_id/sd(r_cvr_id)) |>
   filter(cvr_id < 50) |> 
   ggplot(aes(x = alpha, y = as.character(cvr_id))) +
   stat_halfeye() +
   geom_vline(xintercept = 0, linetype = "dashed", color = "blue") +
   labs(x = expression(alpha), y = "Voter Group", title = "Rasch Model")

p2 <- ber_2pl |> 
  spread_draws(r_cvr_id__alpha[cvr_id,]) |> 
  mutate(alpha = r_cvr_id__alpha/sd(r_cvr_id__alpha)) |> 
  filter(cvr_id < 50) |> 
  ggplot(aes(x = alpha, y = as.character(cvr_id))) +
  stat_halfeye() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue") +
  labs(x = expression(alpha), y = "", title = "2PL Model")

p1 + p2 & theme_medsl()

```

```{r}
#| label: fig-bernoulli-params
#| fig-cap: Discrimination and Difficulty Parameters from the Bernoulli Model

person_pars_2pl <- ranef(ber_2pl, summary = FALSE)$cvr_id[, , "alpha_Intercept"] 
person_sds_2pl <- apply(person_pars_2pl, 1, sd)
item_pars_2pl <- coef(ber_2pl, summary = FALSE)$race
  
# locations
beta <- item_pars_2pl[, , "beta_Intercept"] |>
  as_tibble() |> 
  pivot_longer(cols = everything(), names_to = "race") |> 
  mutate(race = str_remove(race, fixed(", ")) |> str_squish()) |> 
  mutate(nlpar = "beta")

random_races <- beta |> 
  distinct(race) |> 
  slice_sample(n=12) |> 
  bind_rows(tibble(race = c("US PRESIDENT - STATEWIDE", "US SENATE - STATEWIDE"))) |> 
  distinct(race)

# slopes
gamma <- item_pars_2pl[, , "loggamma_Intercept"] |>
  sweep(1, person_sds_2pl, "*") |>
  as_tibble() |> 
  pivot_longer(cols = everything(), names_to = "race") |> 
  mutate(nlpar = "gamma") |> 
  mutate(race = str_remove(race, fixed(", ")) |> str_squish()) 

bind_rows(beta, gamma) |>
  inner_join(random_races) |> 
  mutate(nlpar = factor(nlpar, labels = c("Difficulty", "Discrimination"))) |>
  ggplot(aes(x = value, y = race)) +
  stat_pointinterval() +
  geom_vline(xintercept = 0, color = "blue", linetype = "dashed") +
  facet_wrap(~ nlpar, scales = "free_x") +
  labs(y = "", x = "") +
  theme_bw()
```

## Categorical Outcomes

The categorical model is nearly the same as the Bernoulli model, with some slight modifications. @eq-irt becomes the following, where $F(\cdot)$ is the softmax function, a multivariate generalization of the inverse logit function.

$$
\pi_{jk} = Pr(y_{jk} = c | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - \beta_{k(c)})
$$

In the categorical model, as with all models where the outcome is categorical, a reference category must be set. This is important for the estimates of the discriminatory and difficulty parameters, which are now all in reference to the reference candidate in each race. This makes them difficult to interpret and so I will mostly focus on estimates of the ideal points.

One notable difference in the categorical model are the identification restrictions. The same normalization applies to $\alpha$, but now I must relax the assumption that $\gamma$ is strictly positive, since it is defined in relation to the reference candidate. There is no *a priori* reason to fix one candidate as the least discriminatory, so I cannot fix the sign of $\gamma$. Instead, I let the parameter be unidentified, and use post-processing to flip the signs of $\gamma$ and $\alpha$ to the most modal side of $\gamma$s distribution. Even after this post-processing, some of the distributions of the latent traits still exhibit some bimodality, which I am not sure exactly where to attribute. It could be just that in some draws the values are close to zero, and so the vacillating signs does not clearly identify the model. I expect that adding informative priors on $\gamma$, as discussed in @sec-potential-extensions, could help with this problem.

I estimate this model on a smaller subset of the Colorado data, using only voters from Adams County, a large county that covers the northeast corner of Denver and some rural areas outside of Denver. This constitutes a sample size of $5,949$ voters with $193,501$ choices.

The varying-races, varying-choices structure of election data in does not work well with standard categorical functions because they typically expect a consistent, vector of probabilities of each candidate's selection in a given race. In elections, voters do not participate in the same races in the same election and choose among different candidates. I could estimate all of the parameters independently for each unique race that a voter cast their ballot in, but because I want to share information between races and between voters, those parameters must be estimated jointly. The way I get around the single-length vector requirement of categorical functions is by assigning large negative probabilities to candidates who were ineligible to be voted for in a certain election (e.g., Joe Biden would've had a large negative probability when estimating the function for the Colorado Senate race). Then, the reference category is fixed at 0, and the probabilities for other candidates are allowed to vary. I estimate the model using a bespoke Stan model. More information is provided in @sec-code.

```{r}
#| label: fig-cat-ideals
#| fig-cap: Example Ideal Points from the Categorical Model
#| fig-width: 8
#| fig-height: 6

cat_2pl |> 
  spread_draws(alpha[cvr_id]) |> 
  ungroup() |> 
  mutate(alpha = (alpha - mean(alpha))/sd(alpha)) |> 
  filter(cvr_id < 15) |> 
  ggplot(aes(x = alpha, y = as.character(cvr_id))) +
  stat_halfeye() +
  theme_bw() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue") +
  labs(x = expression(alpha), y = "Voter") +
  theme_medsl()

```

```{r}
#| label: fig-cat-presvote
#| fig-cap: Distribution of Ideal Points of Voters for Trump and Biden voters
#| fig-width: 8

data <- tar_read(data_base_adams) |> 
  # propositions are not quite right
  mutate(candidate = case_when(
    str_detect(race, "PROPOSITION") ~ str_c(race, candidate, sep = " - "),
    TRUE ~ candidate
  ),
  race = str_remove(race, ", "))

# Assign unique IDs to races and candidates
races <- data |> 
  distinct(race) |> 
  arrange(race) |> 
  mutate(race_id = row_number())

candidates <- data |> 
  distinct(race, candidate) |> 
  arrange(race, candidate) |>
  select(candidate) |>
  mutate(candidate_id = row_number())

# Join back to the original data
df <- data |> 
  left_join(races, by = "race") |> 
  left_join(candidates, by = "candidate")

# some races are not classified perfectly in districts rn so they would show up as list-columns (bad)
bad_races <- df |> 
  count(cvr_id, race_id) |> 
  filter(n > 1) |> 
  distinct(race_id) |> 
  pull(race_id)

df <- df |> 
  filter(!(race_id %in% bad_races)) |>
  drop_na(race_id, candidate_id)

pres_choices <- df |>
  filter(office == "US PRESIDENT") |>
  distinct(cvr_id, candidate) |> 
  mutate(candidate = case_match(
    candidate,
    "JOSEPH KISHORE NORISSA SANTA CRUZ" ~ "JOSEPH KISHORE",
    "JORDAN CANCER SCOTT JENNIFER TEPOOL" ~ "JORDAN SCOTT",
    "BILL HAMMONS ERIC BODENSTAB" ~ "BILL HAMMONS",
    "MARK CHARLES ADRIAN WALLACE" ~ "MARK CHARLES",
    .default = candidate
  ))

voters <- df |> 
  select(cvr_id, race_id, candidate_id) |> 
  arrange(race_id, candidate_id) |> 
  distinct(cvr_id) |> 
  mutate(id = row_number()) |> 
  left_join(pres_choices)

joined <- cat_2pl |> 
  spread_draws(alpha[id]) |> 
  ungroup() |> 
  mutate(alpha = alpha/sd(alpha)) |> 
  left_join(voters, join_by(id)) |> 
  drop_na(candidate) 

joined |> 
  filter(candidate %in% c("JOSEPH R BIDEN", "DONALD J TRUMP")) |> 
  ggplot(aes(x = alpha, fill = candidate, y = candidate)) +
  ggridges::geom_density_ridges(scale = 1) +
  scale_fill_discrete(type = c("#F6573E", "#3791FF"), guide = "none") +
  labs(x = expression(alpha), y = "", fill = "Candidate") +
  theme_medsl()

```

```{r}
#| label: fig-cat-presvote-others
#| fig-cap: Distribution of Ideal Points of Voters for Third Party Presidential Candidates
#| fig-height: 8
#| fig-width: 8

joined |> 
  filter(!(candidate %in% c("JOSEPH R BIDEN", "DONALD J TRUMP"))) |> 
  ggplot(aes(x = alpha, y = candidate)) +
  ggridges::geom_density_ridges(fill = "#C0BA79", scale = 1) +
  theme_bw() +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(x = expression(alpha), y = "") +
  theme_medsl()

```

```{r}
#| label: fig-cat-params
#| fig-cap: Discrimination and Difficulty Parameters from the Categorical Model

sds <- cat_2pl |> 
  spread_draws(alpha[cvr_id]) |> 
  ungroup() |> 
  summarise(sd = sd(alpha), .by = ".draw") |> 
  pull(sd)

draws <- cat_2pl |> 
  as_draws_df() |>
  select(starts_with("beta["), starts_with("gamma[")) |> 
  sweep(1, sds, "*") |> 
  mutate(across(everything(), ~ na_if(.x, 0))) |> 
  pivot_longer(cols = everything(), values_drop_na = TRUE) |> 
  separate_wider_delim(cols = name, delim = "[", names = c("parameter", "race_id")) |> 
  separate_wider_delim(cols = race_id, delim = ",", names = c("race_id", "candidate_id")) |> 
  mutate(candidate_id = str_remove(candidate_id, "]")) |> 
  mutate(race_id = as.numeric(race_id),
         candidate_id = as.numeric(candidate_id)) |>
  left_join(races) |> 
  left_join(candidates)

draws |> 
  filter(str_detect(race, "US PRESIDENT")) |> 
  mutate(race = str_remove(race, " - STATEWIDE")) |> 
  mutate(parameter = factor(parameter, labels = c("Difficulty", "Discrimination"))) |>
  ggplot(aes(x = value, y = candidate)) +
  stat_halfeye() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue") +
  facet_wrap(~ parameter, scales = "free_x") +
  theme_bw() +
  labs(x = "", y = "")
```

## Comparison to DIME Scores

# Conclusion {#sec-potential-extensions}

I show, using a novel data source, that a single latent trait does not

## Future Work

I am considering several next steps for this project. First, one choice that @lewis2001 makes in their paper to improve the computational efficiency of the model is to group voters based on their patterns of voting. Since voters are not identified by anything other than their pattern they are interchangeable, and simply multiplying the log-likelihood by the number of voters in that group is sufficient to estimate the model. However, when I briefly experimented with this in the Bernoulli models, the `brms` models had horrible convergence properties. One reason I anticipate this occurs, and is a reason suggested by but not tested in @lewis2001, is that the number of voters in each group (because of the detailed down-ballot data I have) is quite small. I anticipate I will experiment with this process more in the future to increase the speed of estimation, which I anticipate will be about a 25% speedup, assuming the model converges.. Additionally, to speed the computational speed of my categorical model, I need to switch to use the GPU processing supported in Stan, which requires a rewrite of my Stan model. The speedup from this process is unknown but I anticipate it should help some, and will help me take more advantage of MIT's Supercloud platform.

Substantively, there are three extensions that I foresee as useful to this project. First, and most importantly, is setting informative priors on the model. These could be derived from a first pass of a variational inference approach to estimation, followed by the fully Bayesian estimation. Alternatively, they may come from a reformulation of the DIME scores, although as I showed earlier, these are sparse for the races in my data. The advantage of informative priors is simple to see in terms of computational efficiency, but they would also help alleviate issues with bimodality in the categorical model, which is driven by my inability to fix the sign of the discrimination parameter, $\gamma$. Informative priors will help me define this *a priori*.

Second, I have filtered out races where voters could choose more than one candidate to elect. This is relatively common in city council and school board elections and would constitute an interesting extension of this model to make use of the repeated information in the same race about a single voter to more clearly define their latent location in the ideological space. I am not sure how to achieve this computationally.

Third, previous research using IRT models and vote behaviors [@poole_patterns_1991; @rivers2003; @lewis2001] has estimated more than one dimension of the underlying latent ideological dimension. Although there is considerable debate over whether this gap even exists, a full consideration of the data would at least test multiple dimensions and see if this explains the data in a more informative way. In particular, explaining why certain voters cast ballots for third-party candidates or for specific nonpartisan candidates might be better explained by additional dimensions than by the single, "ideological" dimension.

\newpage

# References

::: {#refs}
:::

::: tex
\newpage

\appendix

\renewcommand{\thefigure}{A\arabic{figure}}

\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}

\setcounter{table}{0}
:::

# Appendix {.appendix}

## Code for Estimation {#sec-code}

I use `brms` to estimate the Bernoulli models. For the Rasch model, I use the random-effects formulation of an IRT model, as shown below, where I include a random effect for the race and for each group of voters. This approach is recommended by @burkner_bayesian_2021. I also set relatively uninformative priors on the distributions of each variable.

```{r}
#| eval: false
#| echo: true

bf(
  choice_rep ~ 1 + (1 | race) + (1 | cvr_id), 
  family = brmsfamily("bernoulli", link = "logit")
)

priors <-
      prior("normal(0, 2)", class = "Intercept") +
      prior("normal(0, 3)", class = "sd", group = "cvr_id") +
      prior("normal(0, 3)", class = "sd", group = "race")
```

For the 2PL Bernoulli model, I again follow the recommendation of @burkner_bayesian_2021 and estimate a multivariate, nonlinear model with `brms`. The formula is shown below. I estimate the log of the $\gamma$ parameter and then exponentiate it in the main formula to ensure that it is always positive. Additionally, because of the flexibility enabled in `brms`, I am able to note that the two random effects for race as correlated in the estimates for $\beta$ and $\gamma$. This is notated by the $|i|$ in the formulas. Again, I set relatively uninformative priors on the model parameters.

```{r}
#| eval: false
#| echo: true

bf(
  choice_rep ~ exp(loggamma) * alpha - beta,
  nl = TRUE,
  alpha ~ 0 + (1 | cvr_id),
  beta ~ 1 + (1 |i| race),
  loggamma ~ 1 + (1 |i| race),
  family = brmsfamily("bernoulli", link = "logit")
)

priors <-
      prior("normal(0, 2)", class = "b", nlpar = "beta") +
      prior("normal(0, 1)", class = "b", nlpar = "loggamma") +
      prior("normal(0, 1)", class = "sd", group = "cvr_id", nlpar = "alpha") +
      prior("normal(0, 3)", class = "sd", group = "race", nlpar = "beta") +
      prior("normal(0, 1)", class = "sd", group = "race", nlpar = "loggamma")

```

For the categorical model, I have created a bespoke Stan model to estimate the 2PL IRT model. The varying-races, varying-choices structure of election data in does not work well with the standard Stan function `categorical_logit()` because it expects a single-length row vector of probabilities of each candidate's selection in a given race. I avoid this issue by using two lookup tables, `candidates` which determines if a candidate was eligible to be voted for in a given race $k$ and `eligibility` which determines if a given voter $j$ was eligible to vote in a race $k$. If the candidate was unavailable in a given race, they are assigned a large negative probability of selection, then the first candidate in the race is set as the reference category and has probability 0, then the remaining probabilities are estimated using the IRT setup. Probabilities are only computed for voters who were eligible to vote in the race. I use a non-centered parameterization of $\beta$ to speed estimation, and additionally implement the `reduce_sum` within-chain parallelization recommended by Stan to speed performance. This allows a model for 1200 voters, representing approximately 50,000 observations, to be fit in only an hour on MIT's Supercloud cluster [@reuther2018interactive].

The full model code is shown below.

``` stan
functions {
  real partial_sum(array[, ] int votes_slice,
                   int start, int end,
                   array[] real alpha,
                   array[,] real beta,
                   array[,] real gamma,
                   real mu_beta,
                   array[,] int candidates,
                   array[,] int eligibility) {
    
    int K = dims(candidates)[1];
    int C = dims(candidates)[2];
    int N_slice = end - start + 1;
    array[N_slice] real alpha_slice = alpha[start:end];
    array[N_slice, K] int eligibility_slice = eligibility[start:end, 1:K];
    
    real partial_log_lik = 0;
    
    for (j in 1:N_slice) {
      matrix[K, C] logits = rep_matrix(-1e8, K, C); // Initialize logits
      for (k in 1:K) {
        if (eligibility_slice[j, k] == 1) {
          for (c in 1:C) {
            if (candidates[k, c] == 1) {
              logits[k, c] = gamma[k, c] * alpha_slice[j] - (beta[k, c] + mu_beta);
            }
          }
          partial_log_lik += categorical_logit_lpmf(votes_slice[j, k] | logits[k]');
        }
      }
    }
    return partial_log_lik;
  }
}

data {
  int<lower=0, upper=1> parallelize; // should the code be run using within-chain threading?
  int<lower=1> J; // number of voters
  int<lower=1> K; // number of races
  int<lower=1> C; // number of candidates
  array[K, C] int<lower=0, upper=1> candidates; // candidate availability matrix for each race
  array[J, K] int<lower=0, upper=C> votes; // votes matrix (0 if not voting in that race)
  array[J, K] int<lower=0, upper=1> eligibility; // 1 if voter is eligible for race, 0 otherwise
}

parameters {
  real mu_beta; // mean question difficulty
  array[J] real alpha; // latent ability of voter j - mean latent ability
  array[K, C] real beta_raw;  // difficulty for each race
  array[K, C] real gamma_raw; // discrimination for each race
  real<lower=0> sigma_beta; // scale of difficulties
  real<lower=0> sigma_gamma; // scale of log discrimination
}

transformed parameters {
  array[K, C] real beta = rep_array(0, K, C);  // difficulty for each race
  array[K, C] real gamma = rep_array(0, K, C); // discrimination for each race
  
  for (k in 1:K){
    int reference = 1;
    for (c in 1:C){
      if (candidates[k, c] == 1) {
        if (reference == 1){
          reference = 0;
        } else {
          beta[k, c] = beta_raw[k, c];
          gamma[k, c] = gamma_raw[k, c];
        }
      }
    }
  }
}

model {
  // Priors
  mu_beta ~ student_t(3, 0, 2.5);
  alpha ~ std_normal(); // set to N(0, 1) for identification
  
  for (k in 1:K){
    beta_raw[k, ] ~ normal(0, sigma_beta);
    gamma_raw[k,] ~ normal(0, sigma_gamma);
  }
  
  sigma_beta ~ student_t(3, 0, 2.5);
  sigma_gamma ~ student_t(3, 0, 2.5);
  
  if (parallelize == 1){
    int grainsize = 1;

    target += reduce_sum(partial_sum, votes, grainsize, alpha, beta, gamma, mu_beta, candidates, eligibility);
  }
}
```

## Convergence Diagnostics

```{r}
#| label: fig-trace-ber1
#| fig-cap: Traceplots for 24 random parameters in the Bernoulli Rasch Model
#| fig-height: 12
#| fig-width: 10

vars <- get_variables(ber_1pl) |> sample(size = 24)

mcmc_trace(ber_1pl, pars = vars, ncol = 3)

```

```{r}
#| label: fig-trace-ber2
#| fig-cap: Traceplots for 24 random parameters in the Bernoulli 2PL Model
#| fig-height: 12
#| fig-width: 12

vars <- get_variables(ber_2pl) |> sample(size = 24)

mcmc_trace(ber_2pl, pars = vars, ncol = 3)
```

```{r}
#| label: fig-trace-cat
#| fig-cap: Traceplots for 24 random parameters in the Categorical 2PL Model
#| fig-height: 12
#| fig-width: 8

vars <- summarise_draws(cat_2pl) |> 
  filter(mean > 0.1 | mean < -0.1, !str_detect(variable, "raw")) |> 
  slice_sample(n=24) |> 
  pull(variable)

mcmc_trace(cat_2pl,
           pars = vars, 
           ncol = 3)
```
