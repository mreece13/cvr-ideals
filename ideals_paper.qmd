---
title: "Ideal Point Estimation of Voters Using Cast Vote Records"
author: 
  - name: Mason Reece
    affiliation: 
      - name: Massachussetts Institute of Technology
        department: Political Science
        city: Cambridge
        state: MA
    email: mpreece@mit.edu
    corresponding: true
date: today
date-format: long
bibliography: "references.bib"
linestretch: 1.5
mainfont: Palatino
crossref:
  custom:
    - kind: float
      reference-prefix: Table A
      space-before-numbering: false
      key: atbl
      latex-env: atbl
      caption-location: bottom
    - kind: float
      reference-prefix: Figure A
      space-before-numbering: false
      key: afig
      latex-env: afig
      caption-location: bottom
thanks: |
  I am indebted to feedback on this project from Charles Stewart III, Devin Caughey, Teppei Yamamoto, Fotini Christia, and participants of the MIT Second-Year Paper Workshop. Additionally, I acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper.
abstract: |
  Work in progress.
execute: 
  echo: false
  cache: false
  message: false
  warning: false
format: 
  html:
    toc: true
    embed-resources: true
  pdf:
    toc: false
    number-sections: true
    citecolor: blue
    link-citations: true
    fig-cap-location: bottom
---

# Introduction

How representative is the government of its constituents? This topic has been repeatedly probed in the political science literature [e.g., @caneswrone_out_2002; @bafumi_leapfrog_2010; @kirkland_representation_2022]. One of the many ways that representation is studied in American politics is by comparing the ideologies of politicians those voters who they represent. For politicians, several data sources have been used, that all show high correlation: roll-call votes [@poole_patterns_1991], campaign contributions [@bonica_mapping_2014; @bonica_inferring_2018], and tweets [@barberá2015], among others. These methods rely on the spate of data available about politicians, and their public positions necessitates that much of it be made public to researchers to use in analyses. This is not the case for voters.

The research on voters often uses aggregated measures of voter preferences or survey-based estimates of ideology rather than the actual votes of individuals [for an exception, see @lewis2001]. Survey-based estimates suffer from two related flaws. First, like all surveys, issues of sampling bias, response bias, their temporal relationship with the actual election, and other such problems prevents them from being perfect representations of vote choice. Second, surveys are often unable to ask voters about the full set of choices on their ballot, mostly given space constraints and the difficulty that arises from determining which exact races a voter was eligible to vote in. Aggregated data overcomes these two issues by using the results from an election and being able to look at the results in every race on the ballot. However, with aggregated data we lose the richness of being able to study the same voter up and down the ballot. What if voters are splitting their tickets, vacillate randomly between candidates, or otherwise act in such a way that aggregate measures would miss?

To avoid these problems, I use an original data set of "cast vote records" (CVRs) that reveal anonymous, individual voter choices in each race in the 2020 election. Since these are true votes by people, they do not suffer from the same problems as surveys, and because I can uniquely identify the same voter up and down the ballot, issues of aggregation do not apply here. The downside of using CVRs is that they reveal very little additional information about the voter. There are no names, no voter IDs, no demographic information, or anything that might violate the privacy of a voter. The only thing that could be done would be to attempt to parse the precinct of the voter (which is only available for some counties) and back out aggregate level demographic information on the voters. For this paper, I set this process aside and only work with the anonymized data.

Using this data, I estimate the latent traits of voters that are best described by their vote choices in the election. In previous research, the first dimension of this latent trait is often assumed to be the ideology of the voter, so I refer to this latent trait as ideology henceforth [@clinton2004; @lewis2001; @bonica_mapping_2014; @heckman1996; @bonica_inferring_2018]. These latent traits are interesting in their own right, but I also take the additional step where data is available to compare DIME score estimates of candidate ideology [@bonica_mapping_2014] to aggregate levels of candidate ideology I construct based on those voters who cast their ballots for that candidate. I find that those values are not perfectly correlated, indicating that candidate's behavior while in office is not representative of even those voters who elected them. This research adds to decades of research showing how representative's do not always represent the underlying distribution of beliefs in their districts [@fenno1978; @bafumi_leapfrog_2010].

# Data & Empirical Approach

My data consists of cast vote records (CVRs) collected as part of my summer research with the MIT Election Data & Science Lab (paper forthcoming). CVRs are anonymized records of which choice in each election each voter in the jurisdiction chose on their ballot. For example, a CVR in Colorado would include, for each voter, their choice for President, for the Senate race, for the House district race that voter was located in, and a number of choices in local-level races that the voter was eligible to vote in. The data is fully anonymous and I am not able to identify anything about the voters (although see @kuriwaki2023secret for a cautionary note). This data has been standardized into a common format across the nation.

CVRs were released by some local election administrators in response to increasing calls from election auditors to be able to independently verify the results of the 2020 election. Some states prohibit releasing CVRs in any form, and for other states the ability to do so depended on whether a group requested them from a certain county, whether the county used technology enabling this data to be easily compiled, and whether the county election office had the capacity to even complete the request. It is not known exactly why each county in my data released their data, and so at best this data should be treated as a non-random sample of CVRs. Nevertheless, it still contains almost 1 billion choices in elections at all levels of government and in localities of all kinds from all over the country, representing the choices of roughly 50 million voters. See @fig-map for the full distribution of counties. I remove all uncontested races, and all races where a voter could select more than one candidate (although see @sec-potential-extensions). In addition, I subset my data to a random selection of $100,000$ voters and $2.3$ million choices in Colorado, to make computation tractable. Colorado has some of the best coverage of any state in the data -- nearly every county is represented and the state already has a decent partisan and demographic mix so that I think of it as a good benchmark for how the model would perform in every other state.

![Cast Vote Record Distribution](figs/cd_map.png){#fig-map}

# Model Estimation

Since I am attempting to estimate a continuous, latent, scale using only discrete information on each voter, I follow in the footsteps of other researchers in this topic and estimate an Item-Response Theory (IRT) model [@lewis2001; @clinton2004; @martin2002]. A common specification of the IRT model, reformulated from @jackman2009, is the "two-parameter" model. Subscripts and quantities of interest can be found in @tbl-terms.

$$
\pi_{jk} = Pr(y_{jk} = c | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - \beta_{k(c)})
$$ {#eq-irt}

| Quantity                       | Symbol           |
|--------------------------------|------------------|
| Individual                     | $j = 1, 2, …, J$ |
| Race                           | $k = 1, 2, …, K$ |
| Candidate                      | $c = 1, 2, …, C$ |
| Ideal point                    | $\alpha_j$       |
| Discrimination/Slope Parameter | $\gamma_{k(c)}$  |
| Difficulty/Location Parameter  | $\beta_{k(c)}$   |

: Reference for Relevant Model Quantities and Symbols {#tbl-terms}

Of particular note are the parameters. $\alpha_j \in \mathbb{R}$ is an unobserved attribute of individual $j$ , which is typically considered to be ideology in the election context. $\gamma_{k(c)}$ is an unobserved parameter representing the *item discrimination* of candidate $c$ in race $k$, which is the extent to which the probability of voting for a certain candidate responds to changes in the latent trait $\alpha_j$. When $\gamma_{k(c)} = 1$, this is referred to as the Rasch model. $\beta_{k(c)}$ is an unknown *item difficulty* parameter which just tells us the probability of a certain candidate being chosen, irrespective of the underlying trait. In this context, this is the proportion of the votes that each candidate received. $F(\cdot)$ is a function mapping the equation to the probability line. Most IRT applications, including those used in this context in the past, work with binary data, which simplifies the probability statement above to just be testing whether $y_{jk} = 1$. I begin by manipulating my data to estimate this model as well, but I then also extend the field to work with unordered categorical data, where $y_{jk} = c$.

## Binary Outcomes

To start, I follow the previous literature on this topic @lewis2001 by only focusing on binary choices. That research achieved that by only focusing on propositions, which are inherently encoded as Yes/No choices. However, I want to make use of the full set of choices a voter made in the election, so I create a variable that is a binary 1/0 for if the voter has selected the Republican candidate in the race. This choice means that all ideal points on the right side of the scale will indicate greater likelihood to select the Republican candidate (and thus naturally map to the left/right US political party scale). In theory, a similar variable for the choice of the Democrat would have achieved basically the same results (just with the scale flipped). @eq-irt then becomes

$$
\pi_{jk} = Pr(y_{jk} = 1 | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - \beta_{k(c)})
$$

For the binary model, I set $F(\cdot)$ to be the inverse logistic function. The likelihood can then be expressed as below, given the common independence assumption across voters and races [@clinton2004]. In addition, because there are no unique candidate effects, only effects at the race level, I can drop the indexing by candidate within each race and only reference the race itself.

$$
\mathcal{L} = \prod_{j=1}^J \prod_{k=1}^K \pi_{jk}^{y_{jk}} (1 - \pi_{jk})^{1 - y_{jk}}
$$

As it stands, the model is not identified. Simply put, the scales of $\alpha$ are not set and it can be easily multiplied by any factor or shifted by any constant and the model will behave just the same. Similarly, the sign of $\gamma$ can vacillate with a corresponding switch in the sign of $\alpha$ and result in the same behavior. Therefore, a number of identification restrictions must be imposed. For a more full discussion of these conditions, see @jackman2009, @clinton2004, and @rivers2003. I choose to normalize the latent trait to mean 0 and standard deviation 1, which I impose both in post-processing and by setting a strong standard normal prior. I also impose the restriction that $\gamma$ must always be positive, thus fixing its sign.

I start by fitting a simple Rasch model. I estimate the previous Bernoulli model using `brms`.[^1] All models are run for 4 chains, with 1000 warm-up iterations and then 1000 sampling iterations. Trace plots are too numerous to display in full, but samples can be found in @sec-traces, all of which indicate good convergence. In addition, @afig-rhats plots the $\hat{R}$ value for every parameter in the model. As can be seen in the first two panels, all $\hat{R}$ values are extremely close to 1, indicating the model has converged well.

[^1]: Relevant code snippets can be found in @sec-code.

Rasch models treat each race as equally important in determining the ideal point of a voter, so I only look at those ideal points. I randomly select some voters and plot their estimated ideal points in the left panel of @fig-bernoulli-ideals. These estimates generally make sense; voters far on the left side of 0 cast straight-ticket Democrat ballots whereas voters on the far right cast straight-ticket Republican ballots. Voters in the middle tended to cast more split-ticket ballots. The different exact estimates are due to the differing impact of casting a ballot for the Republican in a specific race versus another, which is driven entirely by the difficulty parameter in the Rasch model but by both the difficulty and discrimination parameter in the 2PL model.

I then proceed after the Rasch model to also fit a 2PL model, which allows the discrimination parameter to vary and be estimated, but requires them to be positive. As can be seen in the right panel of @fig-bernoulli-ideals, the estimates match up closely to the estimates from the Rasch model. That there isn't much difference makes sense given the context of US elections, where most voters choose the same party up and down the ballot, so no single race is likely to be more informative than others on their location on a latent scale. To confirm this, I directly plot the difficulty and discrimination parameters for a random set of races from the 2PL model in @fig-bernoulli-params. Although many of the races are significantly different from 0, most of those races are all quite similarly discriminatory. I don't want to read too much into these parameters, but the lack of wild variation suggests that they are all acting in a similar way to one another.

![Example Ideal Points from the Bernoulli Model](figs/ber_ideals.jpg){#fig-bernoulli-ideals}

![Discrimination and Difficulty Parameters from the Bernoulli Model](figs/ber_params.jpg){#fig-bernoulli-params}

## Categorical Outcomes

The categorical model is nearly the same as the Bernoulli model, with some slight modifications. @eq-irt becomes the following, where $F(\cdot)$ is the softmax function, a multivariate generalization of the inverse logit function.

$$
\pi_{jk(c)} = Pr(y_{jk} = c | \alpha_j, \gamma_{k(c)}, \beta_{k(c)}) = F(\alpha_j \gamma_{k(c)} - \beta_{k(c)})
$$

In the categorical model, as with all models where the outcome is categorical, a reference category must be set. This is important for the estimates of the discriminatory and difficulty parameters, which are now all in reference to the reference candidate in each race. This makes them difficult to interpret and so I will mostly focus on estimates of the ideal points.

One notable difference in the categorical model are the identification restrictions. The same normalization applies to $\alpha$, but now I must relax the assumption that $\gamma$ is strictly positive, since it is defined in relation to the reference candidate. There is no *a priori* reason to fix one candidate as the least discriminatory, so I cannot fix the sign of $\gamma$. Instead, I let the parameter be unidentified, and use post-processing to flip the signs of $\gamma$ and $\alpha$ to the most modal side of $\gamma$s distribution. Even after this post-processing, some of the distributions of the latent traits still exhibit some bimodality, which I am not sure exactly where to attribute. It could be just that in some draws the values are close to zero, and so the vacillating signs does not clearly identify the model. I expect that adding informative priors on $\gamma$, as discussed in @sec-potential-extensions, could help with this problem. Additionally, I intend in a future version of this project to implement the Rotation-Sign-Permutation (RSP) algorithm described by @papastamoulis2022, which is a more precise way to identify latent factors in Bayesian models than the simple approach I have taken here. I expect that RSP will resolve the remaining bimodality seen in the distribution of the latent traits.

I estimate this model on a smaller subset of the Colorado data, using only voters from Adams County, a large county that covers the northeast corner of Denver and some rural areas outside of Denver. This constitutes a sample size of $5,824$ voters with $189,462$ choices.

The varying-races, varying-choices structure of election data in does not work well with standard categorical functions because they typically expect a consistent, vector of probabilities of each candidate's selection in a given race. In elections, voters do not participate in the same races in the same election and choose among different candidates. I could estimate all of the parameters independently for each unique race that a voter cast their ballot in, but because I want to share information between races and between voters, those parameters must be estimated jointly. The way I get around the single-length vector requirement of categorical functions is by assigning large negative probabilities to candidates who were ineligible to be voted for in a certain election (e.g., Joe Biden would've had a large negative probability when estimating the function for the Colorado Senate race). Then, the reference category is fixed at 0, and the probabilities for other candidates are allowed to vary. I estimate the model using a bespoke Stan model. More information is provided in @sec-code.

In @fig-cat-ideals I plot a similar set of example ideal points for 14 random voters (these are NOT the same voters as in @fig-bernoulli-ideals). The categorical model has a lot more variation than the Bernoulli model, which makes sense since we are allowing each voter to have a more complicated representation of their choices, rather than just whether they picked the Republican or not. To further provide evidence that the models are converging to something reasonable, I plot the distribution of all draws of the ideal points for Trump and Biden voters in @fig-cat-presvote. This plot shows that BIden voters are consistently on the left-side of the median, but closer to the middle than their counterparts, Trump voters. Since Trump voters are generally considered a bit more conservative than Biden voters are considered liberal, these results seem to be facially valid given anecdotal knowledge about the American electorate.

![Example Ideal Points from the Categorical Model](figs/cat_ideals.jpg){#fig-cat-ideals}

![Distribution of Ideal Points of Voters for Trump and Biden Voters](figs/cat_aggregated.jpg){#fig-cat-presvote}

One advantage of the categorical model is that I can also replicate these results for all candidates in the presidential race in Colorado. The distributions of ideal points of voters who cast ballots for each of the third-party candidates is shown in @fig-cat-presvote-others. These results generally make sense, candidates like Gloria La Riva (Socialist Party) are on the left side of the scale, while candidates like Don Blankenship (Constitution Party) and Jo Jorgensen (Libertarian Party) are on the right side of the scale. Some candidates, like Rocque de la Fuente have a strongly bimodal distribution which could indicate that they're receiving votes from people on both sides of the aisle. However, as is true for many of these third-party candidates (particularly Kanye West), perhaps the single-dimension assumption imposed by my model doesn't appropriately describe the type of voter who would cast ballots for these candidates.

![Distribution of Ideal Points of Voters for Third Party Presidential Candidates](figs/cat_aggregated_others.jpg){#fig-cat-presvote-others}

For the categorical model, I also plot the discrimination and difficulty parameters for the presidential race in @fig-cat-params. Because both of these parameters are defined in reference to the baseline category (Bill Hammons, in this case), they are challenging to interpret in any meaningful way. In particular, although their credible intervals all cross zero, it looks like votes for Biden or for Trump are more informative (although in contradicting signs) than a vote for Howie Hawkins on a voter's underlying latent trait. This makes sense given our prior knowledge about American voters.

![Discrimination and Difficulty Parameters from the Categorical Model](figs/cat_params.jpg){#fig-cat-params}

## Comparison to DIME Scores

Finally, I want to show how my scores compare to an accepted measure of American ideology -- DIME scores [@bonica_inferring_2018; @bonica_mapping_2014]. DIME scores are focused on candidates instead of voters, which makes them tricky to compare to the scores I have generated. In particular, I cannot back out the locations of candidates from the locations of voters, since their scale is unidentified and there is no way to make that extraction. However, I can compare the "cutpoints" or midpoints between both the DIME scores and between the distributions of voters who have cast votes for each candidate. If those cutpoints seem to be similar, this would provide some rough evidence that my scores correlate highly with DIME scores. As my most flexible model, I use the categorical model and plot the distribution of voters who cast ballots for each candidate in the race in @fig-dime. Additionally, I plot DIME scores on the same axis with a blue dot (normalized to $\mathcal{N}(0, 1)$). The cutpoints can be inferred by looking at the midpoints between the blue points and between the posterior distributions. In the two candidate races it is clear that the location of the cutpoints are quite similar between both my scores and the DIME scores, which provides evidence in favor of my method.

This comparison to DIME scores is limited by the sparsity of data on lower-ballot candidates in the data. Since DIME scores are constructed from public campaign contribution data, some lower-ballot candidates are never captured in the data and it is impossible to construct scores for those candidates. However, my method using CVRs is able to construct these posterior densities for every candidate on the ballot. Using this data, I can answer more detailed questions about the landscape of ideology up and down the ballot in states across the US.

![Comparison with DIME Scores](figs/dime_comparison.jpeg){#fig-dime}

# Next Steps {#sec-potential-extensions}

I am considering several next steps for this project. First, one choice that @lewis2001 makes in their paper to improve the computational efficiency of the model is to group voters based on their patterns of voting. Since voters are not identified by anything other than their pattern they are interchangeable, and simply multiplying the log-likelihood by the number of voters in that group is sufficient to estimate the model. However, when I briefly experimented with this in the Bernoulli models, the `brms` models had horrible convergence properties. One reason I anticipate this occurs, and is a reason suggested by but not tested in @lewis2001, is that the number of voters in each group (because of the detailed down-ballot data I have) is quite small. I anticipate I will experiment with this process more in the future to increase the speed of estimation, which I anticipate will be about a 25% speedup, assuming the model converges.. Additionally, to speed the computational speed of my categorical model, I need to switch to use the GPU processing supported in Stan, which requires a rewrite of my Stan model. The speedup from this process is unknown but I anticipate it should help some, and will help me take more advantage of MIT's Supercloud platform.

Substantively, there are three extensions that I foresee as useful to this project. First, and most importantly, is setting informative priors on the model. These could be derived from a first pass of a variational inference approach to estimation, followed by the fully Bayesian estimation. Alternatively, they may come from a reformulation of the DIME scores, although as I showed earlier, these are sparse for the races in my data. The advantage of informative priors is simple to see in terms of computational efficiency, but they would also help alleviate issues with bimodality in the categorical model, which is driven by my inability to fix the sign of the discrimination parameter, $\gamma$. Informative priors will help me define this *a priori*.

Second, I have filtered out races where voters could choose more than one candidate to elect. This is relatively common in city council and school board elections and would constitute an interesting extension of this model to make use of the repeated information in the same race about a single voter to more clearly define their latent location in the ideological space. I am not sure how to achieve this computationally.

Third, previous research using IRT models and vote behaviors [@poole_patterns_1991; @rivers2003; @lewis2001] has estimated more than one dimension of the underlying latent ideological dimension. Although there is considerable debate over whether this gap even exists, a full consideration of the data would at least test multiple dimensions and see if this explains the data in a more informative way. In particular, explaining why certain voters cast ballots for third-party candidates or for specific nonpartisan candidates might be better explained by additional dimensions than by the single, "ideological" dimension.

\newpage

# References

::: {#refs}
:::

::: tex
\newpage

\appendix

<!-- \renewcommand{\thefigure}{A\arabic{figure}} -->

<!-- \renewcommand{\thetable}{A\arabic{table}} -->

<!-- \setcounter{figure}{0} -->

<!-- \setcounter{table}{0} -->
:::

# Appendix {.appendix}

## Code for Estimation {#sec-code}

I use `brms` to estimate the Bernoulli models. For the Rasch model, I use the random-effects formulation of an IRT model, as shown below, where I include a random effect for the race and for each group of voters. This approach is recommended by @burkner_bayesian_2021. I also set relatively uninformative priors on the distributions of each variable.

```{r}
#| eval: false
#| echo: true

bf(
  choice_rep ~ 1 + (1 | race) + (1 | cvr_id), 
  family = brmsfamily("bernoulli", link = "logit")
)

priors <-
      prior("normal(0, 2)", class = "Intercept") +
      prior("normal(0, 3)", class = "sd", group = "cvr_id") +
      prior("normal(0, 3)", class = "sd", group = "race")
```

For the 2PL Bernoulli model, I again follow the recommendation of @burkner_bayesian_2021 and estimate a multivariate, nonlinear model with `brms`. The formula is shown below. I estimate the log of the $\gamma$ parameter and then exponentiate it in the main formula to ensure that it is always positive. Additionally, because of the flexibility enabled in `brms`, I am able to note that the two random effects for race as correlated in the estimates for $\beta$ and $\gamma$. This is notated by the $|i|$ in the formulas. Again, I set relatively uninformative priors on the model parameters.

```{r}
#| eval: false
#| echo: true

bf(
  choice_rep ~ exp(loggamma) * alpha - beta,
  nl = TRUE,
  alpha ~ 0 + (1 | cvr_id),
  beta ~ 1 + (1 |i| race),
  loggamma ~ 1 + (1 |i| race),
  family = brmsfamily("bernoulli", link = "logit")
)

priors <-
      prior("normal(0, 2)", class = "b", nlpar = "beta") +
      prior("normal(0, 1)", class = "b", nlpar = "loggamma") +
      prior("normal(0, 1)", class = "sd", group = "cvr_id", nlpar = "alpha") +
      prior("normal(0, 3)", class = "sd", group = "race", nlpar = "beta") +
      prior("normal(0, 1)", class = "sd", group = "race", nlpar = "loggamma")

```

For the categorical model, I have created a bespoke Stan model to estimate the 2PL IRT model. The varying-races, varying-choices structure of election data in does not work well with the standard Stan function `categorical_logit()` because it expects a single-length row vector of probabilities of each candidate's selection in a given race. I avoid this issue by using two lookup tables, `candidates` which determines if a candidate was eligible to be voted for in a given race $k$ and `eligibility` which determines if a given voter $j$ was eligible to vote in a race $k$. If the candidate was unavailable in a given race, they are assigned a large negative probability of selection, then the first candidate in the race is set as the reference category and has probability 0, then the remaining probabilities are estimated using the IRT setup. Probabilities are only computed for voters who were eligible to vote in the race. I use a non-centered parameterization of $\beta$ to speed estimation, and additionally implement the `reduce_sum` within-chain parallelization recommended by Stan to speed performance. This allows a model for 1200 voters, representing approximately 50,000 observations, to be fit in only an hour on MIT's Supercloud cluster [@reuther2018interactive].

The full model code is shown below.

``` stan
functions {
  real partial_sum(array[, ] int votes_slice,
                   int start, int end,
                   array[] real alpha,
                   array[,] real beta,
                   array[,] real gamma,
                   real mu_beta,
                   array[,] int candidates,
                   array[,] int eligibility) {
    
    int K = dims(candidates)[1];
    int C = dims(candidates)[2];
    int N_slice = end - start + 1;
    array[N_slice] real alpha_slice = alpha[start:end];
    array[N_slice, K] int eligibility_slice = eligibility[start:end, 1:K];
    
    real partial_log_lik = 0;
    
    for (j in 1:N_slice) {
      matrix[K, C] logits = rep_matrix(-1e8, K, C); // Initialize logits
      for (k in 1:K) {
        if (eligibility_slice[j, k] == 1) {
          for (c in 1:C) {
            if (candidates[k, c] == 1) {
              logits[k, c] = gamma[k, c] * alpha_slice[j] - (beta[k, c] + mu_beta);
            }
          }
          partial_log_lik += categorical_logit_lpmf(votes_slice[j, k] | logits[k]');
        }
      }
    }
    return partial_log_lik;
  }
}

data {
  int<lower=0, upper=1> parallelize;
  int<lower=1> J; // number of voters
  int<lower=1> K; // number of races
  int<lower=1> C; // number of candidates
  array[K, C] int<lower=0, upper=1> candidates;
  array[J, K] int<lower=0, upper=C> votes;
  array[J, K] int<lower=0, upper=1> eligibility;
}

parameters {
  real mu_beta; // mean question difficulty
  array[J] real alpha; // latent ability of voter j - mean latent ability
  array[K, C] real beta_raw;  // difficulty for each race
  array[K, C] real gamma_raw; // discrimination for each race
  real<lower=0> sigma_beta; // scale of difficulties
  real<lower=0> sigma_gamma; // scale of log discrimination
}

transformed parameters {
  array[K, C] real beta = rep_array(0, K, C);  // difficulty for each race
  array[K, C] real gamma = rep_array(0, K, C); // discrimination for each race
  
  for (k in 1:K){
    int reference = 1;
    for (c in 1:C){
      if (candidates[k, c] == 1) {
        if (reference == 1){
          reference = 0;
        } else {
          beta[k, c] = beta_raw[k, c];
          gamma[k, c] = gamma_raw[k, c];
        }
      }
    }
  }
}

model {
  // Priors
  mu_beta ~ student_t(3, 0, 2.5);
  alpha ~ std_normal(); // set to N(0, 1) for identification
  
  for (k in 1:K){
    beta_raw[k, ] ~ normal(0, sigma_beta);
    gamma_raw[k,] ~ normal(0, sigma_gamma);
  }
  
  sigma_beta ~ student_t(3, 0, 2.5);
  sigma_gamma ~ student_t(3, 0, 2.5);
  
  if (parallelize == 1){
    int grainsize = 1;

    target += reduce_sum(partial_sum, votes, grainsize, alpha, beta, gamma, mu_beta, candidates, eligibility);
  }
}
```

## Convergence Diagnostics {#sec-traces}

![Traceplots for 24 random parameters in the Bernoulli Rasch Model](figs/ber_1pl_trace.jpeg){#afig-trace-ber1}

![Traceplots for 24 random parameters in the Bernoulli 2PL Model](figs/ber_2pl_trace.jpeg){#afig-trace-ber2}

![Traceplots for 24 random parameters in the Categorical 2PL Model](figs/cat_2pl_trace.jpeg){#afig-trace-cat}

![Distribution of Gelman-Rubin Diagnostic](figs/rhat_comparison.jpg){#afig-rhats}
